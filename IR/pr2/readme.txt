	Okapi TF and TF-IDF both use term frequency in docs to calculate score, it's reasonable that if query term occurs in doc more time, more likely the term is related.On the other hand,it's not that good as phrase and other language constructs are not be considered. 
	And if we compare between these 2, TF-IDF is slightly worse than TF because with IDF, it consider a 'popular' words less weighted but actually if this query words occurs many times in a specific doc, we should consider the document relative.
	BM25 here looks like the best algorithm, as it uses both query weight and document weight, which overcomes the weakness of TF-IDF said above.
	For language smoothing models, these 2 are worse than others because they calculate the score only by information from the document, like how many times some term occurs in query but not vise versa. This makes the model some what on-sided, assume I entered some query with dog twice, I more likely to find something about dogs, so word 'dog' should get more attention.And what worth to mention is when one term in a query not occurred in one doc but does in many other docs many times. d(i) would still be high which is undesired 
	In all I would say TF and BM25 are slightly better than others but not that significant better.And what's interesting is query 202. only one doc with score 4 and others is 0, which might make it harder to search.
